{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors and Model Evaluation\n",
    "\n",
    "In this programming assignment you will use k Nearest Neighbors (kNN) to build a \"model\" that will estimate the compressive strength of various types of concrete. This assignment has several objectives:\n",
    "\n",
    "1. Implement the kNN algorithm with k=9. Remember...the data + distance function is the model in kNN. In addition to asserts that unit test your code, you should \"test drive\" the model, showing output that a non-technical person could interpret.\n",
    "\n",
    "2. You are going to compare the kNN model above against the baseline model described in the course notes (the mean of the training set's target variable). You should use 10 fold cross validation and Mean Squared Error (MSE):\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum^n_i (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "as the evaluation metric (\"error\"). Refer to the course notes for the format your output should take. Don't forget a discussion of the results.\n",
    "\n",
    "3. use validation curves to tune a *hyperparameter* of the model. \n",
    "In this case, the hyperparameter is *k*, the number of neighbors. Don't forget a discussion of the results.\n",
    "\n",
    "4. evaluate the *generalization error* of the new model.\n",
    "Because you may have just created a new, better model, you need a sense of its generalization error, calculate that. Again, what would you like to see as output here? Refer to the course notes. Don't forget a discussion of the results. Did the new model do better than either model in Q2?\n",
    "\n",
    "5. pick one of the \"Choose Your Own Adventure\" options.\n",
    "\n",
    "Refer to the \"course notes\" for this module for most of this assignment.\n",
    "Anytime you just need test/train split, use fold index 0 for the test set and the remainder as the training set.\n",
    "Discuss any results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The function `parse_data` loads the data from the specified file and returns a List of Lists. The outer List is the data set and each element (List) is a specific observation. Each value of an observation is for a particular measurement. This is what we mean by \"tidy\" data.\n",
    "\n",
    "The function also returns the *shuffled* data because the data might have been collected in a particular order that *might* bias training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [float(value) for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(\"concrete_compressive_strength.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[312.9, 160.5, 0.0, 177.6, 9.6, 916.6, 759.5, 28.0, 52.45]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,030 observations and each observation has 8 measurements. The data dictionary for this data set tells us the definitions of the individual variables (columns/indices):\n",
    "\n",
    "| Index | Variable | Definition |\n",
    "|-------|----------|------------|\n",
    "| 0     | cement   | kg in a cubic meter mixture |\n",
    "| 1     | slag     | kg in a cubic meter mixture |\n",
    "| 2     | ash      | kg in a cubic meter mixture |\n",
    "| 3     | water    | kg in a cubic meter mixture |\n",
    "| 4     | superplasticizer | kg in a cubic meter mixture |\n",
    "| 5     | coarse aggregate | kg in a cubic meter mixture |\n",
    "| 6     | fine aggregate | kg in a cubic meter mixture |\n",
    "| 7     | age | days |\n",
    "| 8     | concrete compressive strength | MPa |\n",
    "\n",
    "The target (\"y\") variable is a Index 8, concrete compressive strength in (Mega?) [Pascals](https://en.wikipedia.org/wiki/Pascal_(unit))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits - n folds\n",
    "\n",
    "With n fold cross validation, we divide our data set into n subgroups called \"folds\" and then use those folds for training and testing. You pick n based on the size of your data set. If you have a small data set--100 observations--and you used n=10, each fold would only have 10 observations. That's probably too small. You want at least 30. At the other extreme, we generally don't use n > 10.\n",
    "\n",
    "With 1,030 observations, n = 10 is fine so we will have 10 folds.\n",
    "`create_folds` will take a list (xs) and split it into `n` equal folds with each fold containing one-tenth of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always use one of the n folds as a test set (and, sometimes, one of the folds as a *pruning* set but not for kNN), and the remaining folds as a training set.\n",
    "We need a function that'll take our n folds and return the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function to give us a train and test datasets where the test set is the fold at index 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_train_test(folds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "Answer the questions above in the space provided below, adding cells as you need to.\n",
    "Put everything in the helper functions and document them.\n",
    "Document everything (what you're doing and why).\n",
    "If you're not sure what format the output should take, refer to the course notes and what they do for that particular topic/algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: kNN\n",
    "\n",
    "Implement k Nearest Neighbors with k = 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"knearestneighbors\"></a> k nearest neighbors algorithm\n",
    "\n",
    "Formal Parameters:\n",
    "**xq** the data point whose target we want to predict\n",
    "\n",
    "**data** the training set\n",
    "\n",
    "**evaluation_metric** How to evaluate the *closeness* of two observations.  It is used closely with distance_function and p.  It is mostly just distance formula in the first 8 indices of a list.\n",
    "\n",
    "**distance_function** How to evaluate distance.  Was mostly just the Minkowski distance.\n",
    "\n",
    "**p** used in the minkowski distance. Defaults to 2.\n",
    "\n",
    "**k** how many neigbors we are using to average the target value.  Defaults to 9.\n",
    "\n",
    "**returns** the mean of the target values of the k nearest neighbors.\n",
    "\n",
    "The k nearest neighbors algorithm predicts target values by identifying relevant features and averaging the target values of the k nearest observations with those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(xq,data,evaluation_metric,distance_function,p=2,k=9):\n",
    "    nearest_neighbors = k_nearest_neighbors_list(xq,data,evaluation_metric,distance_function,p,k)\n",
    "    total = 0\n",
    "    for neighbor in nearest_neighbors:\n",
    "        total += neighbor[8]\n",
    "        \n",
    "    return total/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"knearestneighborslist\"></a> k nearest neighbors list\n",
    "\n",
    "Formal Parameters:\n",
    "**xq** the data point whose target we want to predict\n",
    "\n",
    "**data** the training set\n",
    "\n",
    "**evaluation_metric** How to evaluate the *closeness* of two observations.  It is used closely with distance_function and p.  It is mostly just distance formula in the first 8 indices of a list.\n",
    "\n",
    "**distance_function** How to evaluate distance.  Was mostly just the Minkowski distance.\n",
    "\n",
    "**p** used in the minkowski distance. Defaults to 2.\n",
    "\n",
    "**k** how many neigbors we are using to average the target value.  Defaults to 9.\n",
    "\n",
    "**returns** A list of the k nearest neighbors\n",
    "\n",
    "This function is used directly by the [k nearest neighbors algorithm](#knearestneighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors_list(xq,data,evaluation_metric,distance_function,p=2,k=9):\n",
    "    nearest_neighbors_values = [float('inf')]*k\n",
    "    nearest_neighbors = [0]*k\n",
    "    for data_point in data:\n",
    "        distance = evaluation_metric(data_point,xq,distance_function,p)\n",
    "        update_nearest(data_point,distance,nearest_neighbors,nearest_neighbors_values,k)\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"evaluation_metric\"></a> evaluation metric\n",
    "\n",
    "Formal Parameters:\n",
    "**xq** the data point whose target we want to predict.  A list with 8 elements for features, and usually target at the last index\n",
    "\n",
    "**data_point** a point from the training set.  A list with 9 indices.\n",
    "\n",
    "**distance_function** How to evaluate distance.  Was mostly just the Minkowski distance without the root\n",
    "\n",
    "**p** used in the minkowski distance. Defaults to 2.\n",
    "\n",
    "**returns** The distance between xq and data_point\n",
    "\n",
    "This function is used directly by the [k nearest neighbors algorithm](#knearestneighborslist).  It applies distance formula on the first 8 coordinates of the xq and data_point lists.  Since f(x) = x^p is a continuous, increasing function on R>=0 for all p>0, we don't actually have to take the pth root to compare distances.  I suppose this eases computational strain and computer math errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metric(data_point,xq,distance_function,p=2):\n",
    "    total = 0\n",
    "    for i in range(len(data_point)-1):\n",
    "        total+=distance_function(data_point[i],xq[i],p)\n",
    "    return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(evaluation_metric([1,2,5],[3,4],minkowski_distance_no_root)==8)\n",
    "assert(evaluation_metric([1,2,5],[3,4],minkowski_distance_no_root,1)==4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"minkowski\"></a> Minkowski distance\n",
    "\n",
    "Formal Parameters:\n",
    "**x** a number\n",
    "\n",
    "**y** some other number\n",
    "\n",
    "**p** the exponent of the distance x-y. Defaults to 2.\n",
    "\n",
    "**returns** absolute value of (x-y)^p\n",
    "\n",
    "This function is not actually the minkowski distance, but a helper function.  Used in [evaluation metric](#evaluation_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_distance_no_root(x,y,p=2):\n",
    "    return abs(x-y) **p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(minkowski_distance_no_root(1,5,1)== 4)\n",
    "assert(minkowski_distance_no_root(5,1,1)== 4)\n",
    "assert(minkowski_distance_no_root(1,5)== 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"update_nearest\"></a> update nearest\n",
    "\n",
    "Formal Parameters:\n",
    "**data_point** a data point from the training set\n",
    "\n",
    "**distance** the calculated distance from data_point to xq\n",
    "\n",
    "**nearest_neighbors** A list of the k nearest neighbors to xq (so far).  It is ordered, with closer neighbors at the end of the list, and further neighbors at the beginning\n",
    "\n",
    "**nearest_neighbors_values** A list of the distances of the k nearest neighbors to xq (so far).  It is in decreasing order.\n",
    "\n",
    "**k** how many neighbors\n",
    "\n",
    "**returns** None\n",
    "\n",
    "**modifies** nearest_neighbors and nearest_neighbors_values\n",
    "\n",
    "If data_point is closer to xq than some neighbor(s) in nearest_neighbors, based on distance and nearest_neighbors_values, it will insert the distance and the data_point into the correct locations, and discard the furthest neighbor and distance.  Used to build the final list returned by [k_nearest_neighbors_list](#knearestneighborslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nearest(data_point,distance,nearest_neighbors,nearest_neighbors_values,k):\n",
    "    if distance < nearest_neighbors_values[0]:\n",
    "        nearest_neighbors_values[0] = distance\n",
    "        nearest_neighbors[0] = data_point\n",
    "    i = 0\n",
    "    while i<k-1 and nearest_neighbors_values[i] < nearest_neighbors_values[i+1]:\n",
    "        swap(nearest_neighbors_values,i,i+1)\n",
    "        swap(nearest_neighbors,i,i+1)\n",
    "        i+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing [k_nearest_neighbors_list](#knearestneighborslist) and update_nearest here to avoid conflicts with running on the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = [1,1,1]\n",
    "data1 = [[4,4,4,4],[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3]]\n",
    "answer = [[2,2,2,2],[0,0,0,0],[1,1,1,1]]\n",
    "assert(k_nearest_neighbors_list(xq,data1,evaluation_metric,minkowski_distance_no_root,p=1,k=3)==answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [1]*9\n",
    "data_dist = [float('inf')]*9\n",
    "answer_data = [1]*9\n",
    "answer_dist = [float('inf')]*9\n",
    "answer_dist.pop()\n",
    "answer_dist.append(2)\n",
    "answer_data.pop()\n",
    "answer_data.append(0)\n",
    "\n",
    "update_nearest(0,2,data1,data_dist,9)\n",
    "assert(data1==answer_data)\n",
    "assert(data_dist==answer_dist)\n",
    "\n",
    "data1 = [\"hola\",\"hello\",\"howdy\",\"hi\",\"greetings\"]\n",
    "data_dist = [6,5,3,2,1]\n",
    "data_point = \"hey\"\n",
    "dist = 4\n",
    "answer_data = [\"hello\",\"hey\",\"howdy\",\"hi\",\"greetings\"]\n",
    "answer_dist = [5,4,3,2,1]\n",
    "update_nearest(data_point,dist,data1,data_dist,5)\n",
    "assert(data1==answer_data)\n",
    "assert(data_dist==answer_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"swap\"></a> swap\n",
    "\n",
    "Formal Parameters:\n",
    "**lst** the list to swap values on\n",
    "\n",
    "**index1** an index of the value in lst to swap\n",
    "\n",
    "**index2** the other index of the value in lst to swap\n",
    "\n",
    "**returns** None\n",
    "\n",
    "**modifies** lst\n",
    "\n",
    "A simple helper function to do a swapping operation on a list.  Used by [update_nearest](#update_nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap(lst,index1,index2):\n",
    "    temp = lst[index1]\n",
    "    lst[index1]=lst[index2]\n",
    "    lst[index2] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[0,1,2],[3,4],[5],[6]]\n",
    "swap(lst,0,2)\n",
    "assert(lst==[[5],[3,4],[0,1,2],[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Evaluation vs. The Mean\n",
    "\n",
    "Using Mean Squared Error (MSE) as your evaluation metric, evaluate your implement above and the Null model, the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"nullmodel\"></a> null model\n",
    "\n",
    "Formal Parameters:\n",
    "**data** the training set\n",
    "\n",
    "The null model simply takes the mean of all target values of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_model(data):\n",
    "    total = 0\n",
    "    for data_point in data:\n",
    "        total+= data_point[len(data_point)-1]\n",
    "        \n",
    "    mean = total/len(data)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"mse\"></a> mean squared error\n",
    "\n",
    "Formal Parameters:\n",
    "**train** the training set\n",
    "\n",
    "**test** the set to test against\n",
    "\n",
    "**model** the model to test on.  Model will only take the training set and a data point as formal parameters, so the user should input a lambda function with all the relevant parameters, not the actual function for the model.\n",
    "\n",
    "The mean squared error function makes a prediction on a data point in the test set based on the model run on the training set and that data point.  It then calculates the difference squared, and averages this over all data points in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(train,test,model):\n",
    "    total = 0\n",
    "    for data_point in test:\n",
    "        prediction = model(train,data_point)\n",
    "        actual = data_point[len(data_point)-1]\n",
    "        total+= (prediction - actual)**2\n",
    "    \n",
    "    mean = total/len(test)\n",
    "    return mean\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"nullmodeleval\"></a> null model evaluation\n",
    "\n",
    "**formal parameters**\n",
    "\n",
    "**folds** the folds for data\n",
    "\n",
    "**returns** None\n",
    "\n",
    "prints the error for each of the folds and the mean error accross all the folds in the [baseline model](#nullmodel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null model evaluation\n",
      "fold 0 error: 322.90357000008146\n",
      "fold 1 error: 248.84773030864778\n",
      "fold 2 error: 238.35038767200925\n",
      "fold 3 error: 304.2318391970946\n",
      "fold 4 error: 220.53007860412012\n",
      "fold 5 error: 307.6558775567913\n",
      "fold 6 error: 356.68716016426777\n",
      "fold 7 error: 260.70237138953763\n",
      "fold 8 error: 292.32928143237336\n",
      "fold 9 error: 247.98119463604732\n",
      "mean error: 280.0219490960971\n"
     ]
    }
   ],
   "source": [
    "def null_model_printer(folds):\n",
    "    print(\"null model evaluation\")\n",
    "    null_lambda = lambda train, data_point: null_model(train)\n",
    "    total = 0\n",
    "    for i in range(10):\n",
    "        train, test = create_train_test(folds, i)\n",
    "        error = mean_squared_error(train,test,null_lambda)\n",
    "        print(\"fold \"+str(i)+ \" error: \" + str(error))\n",
    "        total += error\n",
    "\n",
    "    print(\"mean error: \" + str(total/10))\n",
    "\n",
    "null_model_printer(folds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN evaluation\n",
      "fold 0 error: 66.91231301690036\n",
      "fold 1 error: 105.95659733908666\n",
      "fold 2 error: 82.83899000359582\n",
      "fold 3 error: 112.39770213352509\n",
      "fold 4 error: 63.77420997243199\n",
      "fold 5 error: 90.41399377921611\n",
      "fold 6 error: 85.49962636941149\n",
      "fold 7 error: 84.46132404410882\n",
      "fold 8 error: 101.27450837828118\n",
      "fold 9 error: 93.6980547165288\n",
      "mean error: 88.72273197530862\n"
     ]
    }
   ],
   "source": [
    "print(\"kNN evaluation\")\n",
    "kNN_lambda = lambda train, data_point: k_nearest_neighbors(data_point,train,evaluation_metric,minkowski_distance_no_root,p=2,k=9)\n",
    "total = 0\n",
    "for i in range(10):\n",
    "    train, test = create_train_test(folds, i)\n",
    "    error = mean_squared_error(train,test,kNN_lambda)\n",
    "    print(\"fold \"+str(i)+ \" error: \" + str(error))\n",
    "    total += error\n",
    "    \n",
    "print(\"mean error: \" + str(total/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Hyperparameter Tuning\n",
    "\n",
    "Tune the value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"kNNtest\"></a> kNN_tuner\n",
    "\n",
    "\n",
    "**formal parameters**\n",
    "\n",
    "**upper_bound** k ranges from 1 to upper_bound-1\n",
    "\n",
    "**folds** the folds for data\n",
    "\n",
    "**returns** None\n",
    "\n",
    "prints the error for each of the folds and the mean error accross all the folds for each k up to upper bound in the [kNN algorithm](#k_nearest_neighbors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN evaluation for different k\n",
      "k = 1\n",
      "fold 0 error: 69.42476407766993\n",
      "fold 1 error: 73.88831747572816\n",
      "fold 2 error: 80.9708067961165\n",
      "fold 3 error: 86.01133980582529\n",
      "fold 4 error: 61.62753883495146\n",
      "fold 5 error: 64.6175990291262\n",
      "fold 6 error: 86.05543980582523\n",
      "fold 7 error: 69.428245631068\n",
      "fold 8 error: 70.01612815533983\n",
      "fold 9 error: 100.19667864077671\n",
      "mean error: 76.22368582524273\n",
      "k = 2\n",
      "fold 0 error: 56.283593932038826\n",
      "fold 1 error: 99.68995485436886\n",
      "fold 2 error: 74.11385291262135\n",
      "fold 3 error: 88.31174441747571\n",
      "fold 4 error: 54.2759453883495\n",
      "fold 5 error: 66.57155898058254\n",
      "fold 6 error: 76.00316553398056\n",
      "fold 7 error: 68.04372572815538\n",
      "fold 8 error: 97.24017766990289\n",
      "fold 9 error: 83.88065728155343\n",
      "mean error: 76.44143766990291\n",
      "k = 3\n",
      "fold 0 error: 58.36190442286948\n",
      "fold 1 error: 89.46732696871629\n",
      "fold 2 error: 78.8826888888889\n",
      "fold 3 error: 79.33382470334415\n",
      "fold 4 error: 52.492071736785334\n",
      "fold 5 error: 71.60590204962244\n",
      "fold 6 error: 79.55219989212515\n",
      "fold 7 error: 69.05300463861921\n",
      "fold 8 error: 98.33073807982741\n",
      "fold 9 error: 84.61257831715213\n",
      "mean error: 76.16922396979506\n",
      "k = 4\n",
      "fold 0 error: 59.57039368932038\n",
      "fold 1 error: 84.62326595873785\n",
      "fold 2 error: 72.28642973300971\n",
      "fold 3 error: 92.67006080097083\n",
      "fold 4 error: 53.37974550970873\n",
      "fold 5 error: 83.85931432038836\n",
      "fold 6 error: 86.12092815533981\n",
      "fold 7 error: 70.7151348300971\n",
      "fold 8 error: 101.338109223301\n",
      "fold 9 error: 87.96835570388355\n",
      "mean error: 79.25317379247572\n",
      "k = 5\n",
      "fold 0 error: 58.89373941747573\n",
      "fold 1 error: 84.80713891262135\n",
      "fold 2 error: 70.59590166990293\n",
      "fold 3 error: 93.37881829126209\n",
      "fold 4 error: 57.64230590291262\n",
      "fold 5 error: 78.86213157281554\n",
      "fold 6 error: 86.66250368932042\n",
      "fold 7 error: 74.34336858252429\n",
      "fold 8 error: 100.69373122330099\n",
      "fold 9 error: 86.85030139805826\n",
      "mean error: 79.27299406601942\n",
      "k = 6\n",
      "fold 0 error: 59.810259466019424\n",
      "fold 1 error: 90.6675261866235\n",
      "fold 2 error: 72.49407176375405\n",
      "fold 3 error: 99.87701248651567\n",
      "fold 4 error: 61.45025358683927\n",
      "fold 5 error: 79.83147551240558\n",
      "fold 6 error: 94.57665134843579\n",
      "fold 7 error: 77.33528033980582\n",
      "fold 8 error: 99.49150059331171\n",
      "fold 9 error: 89.04486353829559\n",
      "mean error: 82.45788948220061\n",
      "k = 7\n",
      "fold 0 error: 63.12437600554785\n",
      "fold 1 error: 90.64151398850804\n",
      "fold 2 error: 75.04254971270059\n",
      "fold 3 error: 104.74176439468984\n",
      "fold 4 error: 67.2866851991282\n",
      "fold 5 error: 85.19471882306323\n",
      "fold 6 error: 87.7170236378046\n",
      "fold 7 error: 81.6380516742619\n",
      "fold 8 error: 99.2308336635625\n",
      "fold 9 error: 92.81724135129782\n",
      "mean error: 84.74347584505645\n",
      "k = 8\n",
      "fold 0 error: 64.48158429915047\n",
      "fold 1 error: 98.2159196298544\n",
      "fold 2 error: 78.86114277912624\n",
      "fold 3 error: 110.31261350121358\n",
      "fold 4 error: 65.06129115594659\n",
      "fold 5 error: 86.89834789138355\n",
      "fold 6 error: 85.04147662317963\n",
      "fold 7 error: 82.67046559466021\n",
      "fold 8 error: 99.22549114077665\n",
      "fold 9 error: 95.13377791262135\n",
      "mean error: 86.59021105279126\n",
      "k = 9\n",
      "fold 0 error: 66.91231301690036\n",
      "fold 1 error: 105.95659733908666\n",
      "fold 2 error: 82.83899000359582\n",
      "fold 3 error: 112.39770213352509\n",
      "fold 4 error: 63.77420997243199\n",
      "fold 5 error: 90.41399377921611\n",
      "fold 6 error: 85.49962636941149\n",
      "fold 7 error: 84.46132404410882\n",
      "fold 8 error: 101.27450837828118\n",
      "fold 9 error: 93.6980547165288\n",
      "mean error: 88.72273197530862\n",
      "k = 10\n",
      "fold 0 error: 69.57124051456306\n",
      "fold 1 error: 110.08008406796121\n",
      "fold 2 error: 86.12307064077669\n",
      "fold 3 error: 114.11145426213594\n",
      "fold 4 error: 65.34941682524274\n",
      "fold 5 error: 92.17991095145632\n",
      "fold 6 error: 84.81397434951454\n",
      "fold 7 error: 84.00755401941744\n",
      "fold 8 error: 106.98890518446606\n",
      "fold 9 error: 97.56723328155344\n",
      "mean error: 91.07928440970873\n",
      "k = 11\n",
      "fold 0 error: 72.7006050710102\n",
      "fold 1 error: 109.42735105512321\n",
      "fold 2 error: 90.79207497392281\n",
      "fold 3 error: 114.91857827168417\n",
      "fold 4 error: 67.03607532696785\n",
      "fold 5 error: 94.2376502768194\n",
      "fold 6 error: 87.14517396293029\n",
      "fold 7 error: 88.70535472197707\n",
      "fold 8 error: 106.17785463371575\n",
      "fold 9 error: 103.04261146593913\n",
      "mean error: 93.41833297600898\n",
      "k = 12\n",
      "fold 0 error: 71.70182245819846\n",
      "fold 1 error: 105.89203882820927\n",
      "fold 2 error: 89.9221316545307\n",
      "fold 3 error: 118.15749673004319\n",
      "fold 4 error: 71.77299196332254\n",
      "fold 5 error: 99.4213741639698\n",
      "fold 6 error: 87.38629424217906\n",
      "fold 7 error: 93.62809480177995\n",
      "fold 8 error: 109.09096725997844\n",
      "fold 9 error: 105.95472408306362\n",
      "mean error: 95.2927936185275\n",
      "k = 13\n",
      "fold 0 error: 70.1689699373815\n",
      "fold 1 error: 106.27800537140227\n",
      "fold 2 error: 92.3648808525306\n",
      "fold 3 error: 119.0964240880106\n",
      "fold 4 error: 74.74372266904119\n",
      "fold 5 error: 101.18458493134945\n",
      "fold 6 error: 89.91564835985527\n",
      "fold 7 error: 94.85317418854481\n",
      "fold 8 error: 106.54107120124087\n",
      "fold 9 error: 107.88005835008907\n",
      "mean error: 96.30265399494456\n",
      "k = 14\n",
      "fold 0 error: 69.57610214483857\n",
      "fold 1 error: 103.35234841985334\n",
      "fold 2 error: 95.39102180007923\n",
      "fold 3 error: 119.67590947592632\n",
      "fold 4 error: 77.31419712700611\n",
      "fold 5 error: 100.17073759659202\n",
      "fold 6 error: 92.39162875965923\n",
      "fold 7 error: 94.98642427184467\n",
      "fold 8 error: 106.65503247969093\n",
      "fold 9 error: 107.63360674162868\n",
      "mean error: 96.71470088171192\n"
     ]
    }
   ],
   "source": [
    "def kNN_tuner(upper_bound,folds):\n",
    "    print(\"kNN evaluation for different k\")\n",
    "\n",
    "    for j in range(1,upper_bound):\n",
    "        print(\"k = \" + str(j))\n",
    "        kNN_lambda = lambda train, data_point: k_nearest_neighbors(data_point,train,evaluation_metric,minkowski_distance_no_root,p=2,k=j)\n",
    "        total = 0\n",
    "        for i in range(10):\n",
    "            train, test = create_train_test(folds, i)\n",
    "            error = mean_squared_error(train,test,kNN_lambda)\n",
    "            print(\"fold \"+str(i)+ \" error: \" + str(error))\n",
    "            total += error\n",
    "\n",
    "        print(\"mean error: \" + str(total/10))\n",
    "        \n",
    "kNN_tuner(15,folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Generalization Error\n",
    "\n",
    "Analyze and discuss the generalization error of your model with the value of k from Problem 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing kNN on values in the range [[1,14]](#kNNtest), we see that the smallest error comes from k=1, and the error generally increases with k.  kNN outperforms the [null model](#nullmodeleval) for all values of k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Choose your own adventure\n",
    "\n",
    "You have three options for the next part:\n",
    "\n",
    "1. You can implement mean normalization (also called \"z-score standardization\") of the *features*; do not normalize the target, y. See if this improves the generalization error of your model (middle).\n",
    "\n",
    "2. You can implement *learning curves* to see if more data would likely improve your model (easiest).\n",
    "\n",
    "3. You can implement *weighted* kNN and use the real valued GA to choose the weights. weighted kNN assigns a weight to each item in the Euclidean distance calculation. For two points, j and k:\n",
    "$$\\sqrt{\\sum w_i (x^k_i - x^j_i)^2}$$\n",
    "\n",
    "You can think of normal Euclidean distance as the case where $w_i = 1$ for all features  (ambitious, but fun...you need to start EARLY because it takes a really long time to run).\n",
    "\n",
    "The easier the adventure the more correct it must be..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"zscore\"></a> z score standardization\n",
    "\n",
    "Formal Parameters:\n",
    "**data** The data\n",
    "\n",
    "**returns** None\n",
    "\n",
    "Standardizes the indices 0-7 of each row in data according to the formula Z=(X-mean)/standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting code and discussion\n",
    "import copy\n",
    "def z_score_standardization(data):\n",
    "    means= [0]*8\n",
    "    standard_deviations = [0]*8\n",
    "    standardized_data = copy.deepcopy(data)\n",
    "    \n",
    "    for observation in data:\n",
    "        for i in range(len(observation)-1):\n",
    "            means[i]+=observation[i]\n",
    "            \n",
    "    for i in range(len(means)):\n",
    "        means[i]/=len(data)\n",
    "        \n",
    "    for observation in data:\n",
    "        for i in range(len(observation)-1):\n",
    "            standard_deviations[i]+=(observation[i]-means[i])**2\n",
    "    \n",
    "    for i in range(len(standard_deviations)):\n",
    "        standard_deviations[i]/=len(data)\n",
    "        standard_deviations[i] = standard_deviations[i]**.5\n",
    "        \n",
    "    for observation in standardized_data:\n",
    "        for i in range(len(observation)-1):\n",
    "            observation[i] = (observation[i]-means[i])/standard_deviations[i]\n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = z_score_standardization(data)\n",
    "folds = create_folds(new_data, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null model evaluation\n",
      "fold 0 error: 322.90357000008146\n",
      "fold 1 error: 248.84773030864778\n",
      "fold 2 error: 238.35038767200925\n",
      "fold 3 error: 304.2318391970946\n",
      "fold 4 error: 220.53007860412012\n",
      "fold 5 error: 307.6558775567913\n",
      "fold 6 error: 356.68716016426777\n",
      "fold 7 error: 260.70237138953763\n",
      "fold 8 error: 292.32928143237336\n",
      "fold 9 error: 247.98119463604732\n",
      "mean error: 280.0219490960971\n"
     ]
    }
   ],
   "source": [
    "null_model_printer(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN evaluation for different k\n",
      "k = 1\n",
      "fold 0 error: 59.95056796116507\n",
      "fold 1 error: 89.42234660194178\n",
      "fold 2 error: 86.52787864077669\n",
      "fold 3 error: 93.5237776699029\n",
      "fold 4 error: 57.8706582524272\n",
      "fold 5 error: 64.92879417475729\n",
      "fold 6 error: 90.66249708737863\n",
      "fold 7 error: 60.3011300970874\n",
      "fold 8 error: 62.45907184466021\n",
      "fold 9 error: 92.31745922330096\n",
      "mean error: 75.79641815533981\n",
      "k = 2\n",
      "fold 0 error: 54.066966747572806\n",
      "fold 1 error: 105.1210395631067\n",
      "fold 2 error: 83.79595849514558\n",
      "fold 3 error: 96.59036067961165\n",
      "fold 4 error: 58.64883932038833\n",
      "fold 5 error: 62.91974805825244\n",
      "fold 6 error: 80.47399441747572\n",
      "fold 7 error: 64.60396577669906\n",
      "fold 8 error: 73.32246432038836\n",
      "fold 9 error: 90.85974417475724\n",
      "mean error: 77.04030815533977\n",
      "k = 3\n",
      "fold 0 error: 58.59115825242719\n",
      "fold 1 error: 96.12142761596544\n",
      "fold 2 error: 79.50537971952538\n",
      "fold 3 error: 71.73220377562029\n",
      "fold 4 error: 50.406561920172585\n",
      "fold 5 error: 70.23700312837109\n",
      "fold 6 error: 80.99009795037757\n",
      "fold 7 error: 69.35562653721682\n",
      "fold 8 error: 79.39534617044227\n",
      "fold 9 error: 85.99970571736789\n",
      "mean error: 74.23345107874864\n",
      "k = 4\n",
      "fold 0 error: 65.32769757281554\n",
      "fold 1 error: 91.18837281553401\n",
      "fold 2 error: 72.97107026699028\n",
      "fold 3 error: 81.75217251213593\n",
      "fold 4 error: 50.75840916262135\n",
      "fold 5 error: 71.08633622572818\n",
      "fold 6 error: 84.96697075242722\n",
      "fold 7 error: 62.93338640776698\n",
      "fold 8 error: 77.43113689320388\n",
      "fold 9 error: 84.46995358009711\n",
      "mean error: 74.28855061893205\n",
      "k = 5\n",
      "fold 0 error: 61.81821495145632\n",
      "fold 1 error: 90.81210442718447\n",
      "fold 2 error: 69.07313895145631\n",
      "fold 3 error: 79.26240862135923\n",
      "fold 4 error: 54.58569328155339\n",
      "fold 5 error: 75.14094699029128\n",
      "fold 6 error: 91.06735984466019\n",
      "fold 7 error: 63.092218058252406\n",
      "fold 8 error: 82.72228023300968\n",
      "fold 9 error: 86.28058155339808\n",
      "mean error: 75.38549469126215\n",
      "k = 6\n",
      "fold 0 error: 59.307203586839286\n",
      "fold 1 error: 91.11977009169355\n",
      "fold 2 error: 68.33944730312838\n",
      "fold 3 error: 80.51230752427182\n",
      "fold 4 error: 56.43179150485436\n",
      "fold 5 error: 76.88059646709816\n",
      "fold 6 error: 95.51983101402375\n",
      "fold 7 error: 69.34171200107873\n",
      "fold 8 error: 78.47959018338726\n",
      "fold 9 error: 85.05497216828479\n",
      "mean error: 76.09872218446601\n",
      "k = 7\n",
      "fold 0 error: 68.91509536358231\n",
      "fold 1 error: 86.22947451951659\n",
      "fold 2 error: 68.1712097879929\n",
      "fold 3 error: 83.96432672874968\n",
      "fold 4 error: 53.999671408757685\n",
      "fold 5 error: 84.37179144045966\n",
      "fold 6 error: 94.10871723796313\n",
      "fold 7 error: 70.62092282544086\n",
      "fold 8 error: 80.64358918169209\n",
      "fold 9 error: 90.92612629284727\n",
      "mean error: 78.19509247870022\n",
      "k = 8\n",
      "fold 0 error: 74.8499328731796\n",
      "fold 1 error: 88.91363563410194\n",
      "fold 2 error: 70.7423239987864\n",
      "fold 3 error: 85.66815348907764\n",
      "fold 4 error: 54.702357160194175\n",
      "fold 5 error: 82.90808746966023\n",
      "fold 6 error: 91.80755365594659\n",
      "fold 7 error: 75.33364243021845\n",
      "fold 8 error: 81.6020103762136\n",
      "fold 9 error: 96.31887472694174\n",
      "mean error: 80.28465718143204\n",
      "k = 9\n",
      "fold 0 error: 80.33075176794917\n",
      "fold 1 error: 98.73031797914422\n",
      "fold 2 error: 75.40286221982501\n",
      "fold 3 error: 91.21989790243313\n",
      "fold 4 error: 55.23311043988976\n",
      "fold 5 error: 79.90469935275078\n",
      "fold 6 error: 93.6585266211195\n",
      "fold 7 error: 78.70073610212157\n",
      "fold 8 error: 84.83286184825599\n",
      "fold 9 error: 95.02594549922085\n",
      "mean error: 83.30397097327099\n",
      "k = 10\n",
      "fold 0 error: 78.92184609708734\n",
      "fold 1 error: 100.65650635922333\n",
      "fold 2 error: 79.88478536893201\n",
      "fold 3 error: 91.45066707766988\n",
      "fold 4 error: 59.19541529126215\n",
      "fold 5 error: 78.82766571844661\n",
      "fold 6 error: 94.52354819417474\n",
      "fold 7 error: 83.97930157281559\n",
      "fold 8 error: 86.11510010679615\n",
      "fold 9 error: 94.42865169902913\n",
      "mean error: 84.7983487485437\n",
      "k = 11\n",
      "fold 0 error: 80.23438011714677\n",
      "fold 1 error: 106.26484713953296\n",
      "fold 2 error: 80.6045346626013\n",
      "fold 3 error: 94.72904533418924\n",
      "fold 4 error: 61.180922289978334\n",
      "fold 5 error: 83.28742327689963\n",
      "fold 6 error: 95.3485516488807\n",
      "fold 7 error: 87.94112661477976\n",
      "fold 8 error: 88.57940979699912\n",
      "fold 9 error: 93.97736989488891\n",
      "mean error: 87.21476107758967\n",
      "k = 12\n",
      "fold 0 error: 76.51082229638618\n",
      "fold 1 error: 110.47199270496228\n",
      "fold 2 error: 79.83689413430419\n",
      "fold 3 error: 97.4116907092772\n",
      "fold 4 error: 62.368417502696836\n",
      "fold 5 error: 84.92076785329016\n",
      "fold 6 error: 98.91493011057179\n",
      "fold 7 error: 87.85838899002155\n",
      "fold 8 error: 88.74421324838188\n",
      "fold 9 error: 93.9542290925027\n",
      "mean error: 88.09923466423949\n",
      "k = 13\n",
      "fold 0 error: 75.18684850347557\n",
      "fold 1 error: 107.57375899925317\n",
      "fold 2 error: 79.32911174240247\n",
      "fold 3 error: 99.0726190555524\n",
      "fold 4 error: 62.294023708852755\n",
      "fold 5 error: 88.86780183259611\n",
      "fold 6 error: 101.57729111277074\n",
      "fold 7 error: 89.1590838513242\n",
      "fold 8 error: 85.2007477796289\n",
      "fold 9 error: 95.74545773539379\n",
      "mean error: 88.40067443212502\n",
      "k = 14\n",
      "fold 0 error: 75.60529279770161\n",
      "fold 1 error: 105.64503662076476\n",
      "fold 2 error: 78.7792777342976\n",
      "fold 3 error: 101.17172610461662\n",
      "fold 4 error: 58.977143520903496\n",
      "fold 5 error: 90.28159823657617\n",
      "fold 6 error: 107.98657571824847\n",
      "fold 7 error: 92.194050995641\n",
      "fold 8 error: 85.9262032791757\n",
      "fold 9 error: 94.46127327125022\n",
      "mean error: 89.10281782791756\n"
     ]
    }
   ],
   "source": [
    "kNN_tuner(15,folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, comparing this to the kNN test done on the [original data](#kNNtest), we get a slightly lower error for each k. The [null model](#nullmodeleval) is unaffected.  I was unsure of whether to normalize the entire data set at once, or normalize each training and testing data set one at a time.  I chose to do the former."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Difficulty\n",
    "\n",
    "My main concern is that the [error output](#kNNtest) is in actual values, not percentages, like in the book.  Usually, when I find percent error, I use the formula 100*(predicted-actual)/actual %.  However, I didn't see a place to use that in conjunction with [MSE](#mse).  I'm also unsure if I'm doing [z score standardization](#zscore) correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
