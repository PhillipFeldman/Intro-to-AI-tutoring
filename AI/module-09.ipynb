{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "data = parse_data(\"agaricus-lepiota.data\")\n",
    "\n",
    "len(data[0])\n",
    "\n",
    "len(data)\n",
    "\n",
    "\n",
    "\n",
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "folds = create_folds(data, 10)\n",
    "\n",
    "len(folds)\n",
    "\n",
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test\n",
    "\n",
    "training, test = create_train_test(folds, 0)\n",
    "\n",
    "len(training)\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"count_attributes\"></a> count_attributes\n",
    "\n",
    "Formal Parameters:\n",
    "\n",
    "**data** The data to train the nbc on\n",
    "\n",
    "**returns** A dictionary mapping each attribute to how many times it appears with `'e'` and `'p'`\n",
    "\n",
    "used in [classifier_maker](#classifier_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_attributes(data):\n",
    "    count_dict = {}\n",
    "    for j in data:\n",
    "        for i in range(1,len(j)):\n",
    "            if not (i,j[i]) in count_dict.keys():\n",
    "                count_dict[(i,j[i])] = {'e':0,'p':0}\n",
    "            count_dict[(i,j[i])][j[0]] += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1 = [['e','a','b','c']]\n",
    "\n",
    "assert(count_attributes(test_data1) == {(1,'a'):{'e':1,'p':0},\\\n",
    "                                       (2,'b'):{'e':1,'p':0},\\\n",
    "                                       (3,'c'):{'e':1,'p':0}})\n",
    "\n",
    "test_data2 = [\n",
    "    ['e','a','b','c'],\n",
    "    ['p','a','b','c']]\n",
    "\n",
    "assert(count_attributes(test_data2) == {(1,'a'):{'e':1,'p':1},\\\n",
    "                                       (2,'b'):{'e':1,'p':1},\\\n",
    "                                       (3,'c'):{'e':1,'p':1}}) \n",
    "    \n",
    "test_data3 = [\n",
    "    ['e','a','b','c'],\n",
    "    ['p','a','b','c'],\n",
    "    ['e','a','b','c'],\n",
    "    ['e','b','c','a'],\n",
    "]\n",
    "assert(count_attributes(test_data3) == {(1,'a'):{'e':2,'p':1},\n",
    "                                        (2,'b'):{'e':2,'p':1},\n",
    "                                        (3,'c'):{'e':2,'p':1},\n",
    "                                       (1,'b'):{'e':1,'p':0},\n",
    "                                       (2,'c'):{'e':1,'p':0},\n",
    "                                       (3,'a'):{'e':1,'p':0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"count_classes\"></a> count_classes\n",
    "\n",
    "Formal Parameters:\n",
    "\n",
    "**data** The data to train the nbc on\n",
    "\n",
    "**returns** the tuple (count_dict: a dict counting each class, total: the total amount of observations)\n",
    "\n",
    "used in [classifier_maker](#classifier_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(data):\n",
    "    count_dict = {}\n",
    "    total = 0\n",
    "    for j in data:\n",
    "        total+=1\n",
    "        if j[0] in count_dict.keys():\n",
    "            count_dict[j[0]]+=1\n",
    "        else:\n",
    "            count_dict[j[0]]=1\n",
    "    return count_dict, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"classifier_maker\"></a> classifier_maker\n",
    "\n",
    "Formal Parameters:\n",
    "\n",
    "**data** The data to train the nbc on\n",
    "\n",
    "**smoothing** Whether to smooth the data to deal with missing values\n",
    "\n",
    "**returns** classify (not to be confused with [classify](#classify)) a nested function meant to be the nbc.  \n",
    "Its formal parameters are:\n",
    "        **date_points** a nested list with each inner list representing an observation\n",
    "        **labeled** a boolean that determines whether the data_points  is labeled, helping to clean the data\n",
    "        **returns** a List of Tuples. Each Tuple has the best class in the first position and\n",
    "        a dict with a key for everypossible class label and the associated normalized probability\n",
    "\n",
    "The nbc is trained on the data, but is only half evaluated.  Since an nbc will grow exponentially with the size of an observation, I chose to make it a higher order function that will lazily classify data when called instead of making a giant map for every possibility of observations, saving time and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_maker(data,smoothing):\n",
    "    class_dict,total = count_classes(data)\n",
    "    total_e = class_dict['e']\n",
    "    total_p = class_dict['p']\n",
    "    if smoothing:\n",
    "        total_e+=1\n",
    "        total_p+=1\n",
    "    count_dict = count_attributes(data)\n",
    "    def classify(data_points,labeled=True):\n",
    "        if not labeled:\n",
    "            data_points = deepcopy(data_points)\n",
    "            for data in data_points:\n",
    "                data = [''] + data \n",
    "        l = []\n",
    "        for data_point in data_points:\n",
    "            prob_e = total_e/total\n",
    "            prob_p = total_p/total\n",
    "            for i in range(1,len(data_point)):\n",
    "                curr_p = count_dict[(i,data_point[i])]['p']\n",
    "                curr_e = count_dict[(i,data_point[i])]['e']\n",
    "                curr_total = curr_p+curr_e\n",
    "                if smoothing:\n",
    "                    curr_p+=1\n",
    "                    curr_e+=1\n",
    "                prob_e*=(curr_e/total_e)\n",
    "                prob_p*=(curr_p/total_p)\n",
    "            prob_total = prob_e+prob_p\n",
    "            prob_e = prob_e/prob_total\n",
    "            prob_p = prob_p/prob_total\n",
    "            if prob_e > prob_p:\n",
    "                l.append(('e',{'e':prob_e,'p':prob_p}))\n",
    "            else:\n",
    "                l.append(('p',{'e':prob_e,'p':prob_p}))\n",
    "        return l   \n",
    "    return classify\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"train\"></a> train\n",
    "\n",
    "Formal Parameters:\n",
    "\n",
    "**data** The data to train the nbc on\n",
    "\n",
    "**smoothing** Whether to smooth the data to deal with missing values\n",
    "\n",
    "**returns** A call to [classifier_maker](#classifier_maker), which returns the nbc as a higher order function.\n",
    "\n",
    "The nbc is trained on the data, but is only half evaluated.  Since an nbc will grow exponentially in the size of an observation, I chose to make it a higher order function that will lazily classify data when called instead of making a giant map for every possibility of observations, saving time and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, smoothing = True):\n",
    "    return classifier_maker(data,smoothing)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('p', {'e': 0.47058823529411764, 'p': 0.5294117647058824}), ('p', {'e': 0.47058823529411764, 'p': 0.5294117647058824}), ('p', {'e': 0.47058823529411764, 'p': 0.5294117647058824}), ('e', {'e': 1.0, 'p': 0.0})]\n",
      "[('p', {'e': 0.4576271186440678, 'p': 0.5423728813559322}), ('p', {'e': 0.4576271186440678, 'p': 0.5423728813559322}), ('p', {'e': 0.4576271186440678, 'p': 0.5423728813559322}), ('e', {'e': 0.6666666666666666, 'p': 0.3333333333333333})]\n"
     ]
    }
   ],
   "source": [
    "test_data3 = [\n",
    "    ['e','a','b','c'],\n",
    "    ['p','a','b','c'],\n",
    "    ['e','a','b','c'],\n",
    "    ['e','b','c','a'],\n",
    "]\n",
    "nbc = train(test_data3,False)\n",
    "print(nbc(test_data3))\n",
    "nbc = train(test_data3,True)\n",
    "print(nbc(test_data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"classify\"></a> classify\n",
    "\n",
    "Formal Parameters:\n",
    "\n",
    "**nbc** The nbc, a higher order function\n",
    "\n",
    "**observations**  a list of attribute values observed\n",
    "\n",
    "**labeled** Whether the observations are labeled with a classification, defaults to `True`\n",
    "\n",
    "**returns** `'p'` or `'e'`, if the mushroom is poisonous or edible, based on the observations.\n",
    "\n",
    "This classifies observations based on a nbc.  The labeled parameter helps determine if we are testing or not, and helps the function clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(nbc,observations,labeled=True):\n",
    "    return nbc(observations,labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"evaluate\"></a> evaluate\n",
    "\n",
    "Formal Parameters:\n",
    "\n",
    "**nbc** The nbc, a higher order function\n",
    "\n",
    "**test**  A list of lists of test data\n",
    "\n",
    "**model** A higher order function, but actually just [classify](#classify)\n",
    "\n",
    "**returns** The error rate: amount of errors/total\n",
    "\n",
    "This determines the error rate of the nbc on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(nbc,test,model,labeled):\n",
    "    total = len(test)\n",
    "    error = 0\n",
    "    for data_point in test:\n",
    "        prediction = model(nbc,[data_point],labeled)\n",
    "        prediction = prediction[0][0]\n",
    "        actual = data_point[0]\n",
    "        if actual!=prediction:\n",
    "            error+=1\n",
    "    \n",
    "    rate = error/total\n",
    "    return rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"cross_validate\"></a> cross_validate\n",
    "\n",
    "Formal Parameters:\n",
    "\n",
    "**folds** A decision tree as a nested dict\n",
    "\n",
    "**smoothing**  Whether the nbc should perform smoothing to deal with missing values\n",
    "\n",
    "**labeled** Whether the data is labeled or not.  Used by [classify](#classify)\n",
    "\n",
    "**returns** The average error rate of 10 folds of cross validation\n",
    "\n",
    "**prints** Average error rate, variance\n",
    "\n",
    "This determines the average error rate and variance of the different nbc over 10 folds of cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(folds,smoothing = True,labeled=True):\n",
    "    print(\"nbc evaluation with smoothing = \"+ str(smoothing))\n",
    "    total = 0\n",
    "    rates = []\n",
    "    variance = 0\n",
    "    for i in range(10):\n",
    "        training, test = create_train_test(folds, i)\n",
    "        nbc = train(training,smoothing)\n",
    "        error = evaluate(nbc,test,classify,labeled)\n",
    "        print(\"fold \"+str(i)+ \" error rate: \" + str(100*error)+\"%\")\n",
    "        total += error\n",
    "        rates.append(error)\n",
    "      \n",
    "    mean = total/10\n",
    "    for r in rates:\n",
    "        variance += (r-mean)**2\n",
    "        \n",
    "    variance /=9\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"mean error rate: \" + str(100*mean)+\"%\")\n",
    "    print(\"variance: \" + str(10000*variance) +\"%\")\n",
    "    return 100*total/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbc evaluation with smoothing = True\n",
      "fold 0 error rate: 4.797047970479705%\n",
      "fold 1 error rate: 4.551045510455105%\n",
      "fold 2 error rate: 5.289052890528905%\n",
      "fold 3 error rate: 4.674046740467404%\n",
      "fold 4 error rate: 4.433497536945813%\n",
      "fold 5 error rate: 4.1871921182266005%\n",
      "fold 6 error rate: 3.9408866995073892%\n",
      "fold 7 error rate: 4.1871921182266005%\n",
      "fold 8 error rate: 5.0492610837438425%\n",
      "fold 9 error rate: 4.556650246305419%\n",
      "mean error rate: 4.566587291488679%\n",
      "variance: 0.1685584432940179%\n",
      "nbc evaluation with smoothing = False\n",
      "fold 0 error rate: 0.4920049200492005%\n",
      "fold 1 error rate: 0.7380073800738007%\n",
      "fold 2 error rate: 0.24600246002460024%\n",
      "fold 3 error rate: 0.24600246002460024%\n",
      "fold 4 error rate: 0.12315270935960591%\n",
      "fold 5 error rate: 0.24630541871921183%\n",
      "fold 6 error rate: 0.3694581280788177%\n",
      "fold 7 error rate: 0.0%\n",
      "fold 8 error rate: 0.12315270935960591%\n",
      "fold 9 error rate: 0.49261083743842365%\n",
      "mean error rate: 0.3076697023127867%\n",
      "variance: 0.04792399804009961%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3076697023127867"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = create_folds(data,10)\n",
    "cross_validate(folds)\n",
    "cross_validate(folds,smoothing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that without smoothing, the nbc has about a 10x lower mean error rate and 4x lower variance.  I assume this is because smoothing is used to handle missing data, and this particular data set has relatively few missing values, and all the missing values are for one attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
